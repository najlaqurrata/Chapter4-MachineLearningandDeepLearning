{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp2ESUxQv25r"
      },
      "source": [
        "## **BONUS TASK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMLiaSEUgNcg"
      },
      "source": [
        "This problem description is the same as Task 3. But there are some technical challenges that you have to choose one.\n",
        "\n",
        "1.   Compare 3 different configurations while your model is wider/deeper. Show and explain the performance result.\n",
        "2.   Compare 3 configurations for different Loss Function. Show and explain your performance result.\n",
        "3. Compare 3 configurations for the activation function. Show and explain your performance result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF--Gg5Kp8X8"
      },
      "outputs": [],
      "source": [
        "#Import all libraries\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5g_HRlKkz0k"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset for the data loader\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-rD0pp9O2dy"
      },
      "source": [
        "Comparing diverse configurations for activation functions is a crucial step in model development. In this experiment, we will train and evaluate three distinct models employing different activation function approaches.\n",
        "\n",
        "\n",
        "*   Model 1: ReLU activation function. This activation function is applied to the first layer (fc1) and aids in accelerating convergence during training.\n",
        "*   Model 2: Sigmoid activation function. This function adjusts the output of the first layer to a range between 0 and 1, proving useful for binary classification tasks.\n",
        "*   Model 3: Softmax activation function. This function is applied to the output layer (fc2) to generate a probability distribution, particularly beneficial for multi-class classification tasks.\n",
        "\n",
        "\n",
        "Here is the code implementation for the three models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT0TbP5xkJhB"
      },
      "outputs": [],
      "source": [
        "# Model 1: ReLU activation function\n",
        "\n",
        "class Model1(nn.Module): # Define a neural network model named Model1 that inherits from nn.Module\n",
        "  def __init__(self): # Constructor method (__init__) initializes the object when an instance is created\n",
        "      super(Model1, self).__init__() # Initialize the Model1 class as a subclass of nn.Module\n",
        "      self.fc1 = nn.Linear(28*28, 128) # Input size: 28*28, Output size: 128\n",
        "      self.fc2 = nn.Linear(128, 10) # Input size: 128, Output size: 10\n",
        "\n",
        "  def forward(self, x): # Forward method defines the forward pass of the neural network\n",
        "      x = x.view(-1, 28*28) # Reshape the input tensor x to have dimensions (-1, 28*28)\n",
        "      x = torch.relu(self.fc1(x)) # Apply the ReLU activation function to the output of fc1\n",
        "      x = self.fc2(x) # Pass the result through fc2 (no activation function here)\n",
        "      return x # Return the final output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLSTyF_V2n2-"
      },
      "outputs": [],
      "source": [
        "# Model 2: Sigmoid activation function\n",
        "\n",
        "class Model2(nn.Module): # Define a neural network model named Model2 that inherits from nn.Module\n",
        "  def __init__(self): # Constructor method (__init__) initializes the object when an instance is created\n",
        "      super(Model2, self).__init__() # Initialize the Model2 class as a subclass of nn.Module\n",
        "      self.fc1 = nn.Linear(28*28, 128) # Input size: 28*28, Output size: 128\n",
        "      self.fc2 = nn.Linear(128, 10) # Input size: 128, Output size: 10\n",
        "\n",
        "  def forward(self, x):  # Forward method defines the forward pass of the neural network\n",
        "      x = x.view(-1, 28*28) # Reshape the input tensor x to have dimensions (-1, 28*28)\n",
        "      x = torch.sigmoid(self.fc1(x)) # Apply the sigmoid activation function to the output of fc1\n",
        "      x = self.fc2(x) # Pass the result through fc2 (no activation function here)\n",
        "      return x # Return the final output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osdSNiWB3WO_"
      },
      "outputs": [],
      "source": [
        "# Model 3: Softmax activation function\n",
        "\n",
        "class Model3(nn.Module): # Define a neural network model named Model3 that inherits from nn.Module\n",
        "  def __init__(self): # Constructor method (__init__) initializes the object when an instance is created\n",
        "      super(Model3, self).__init__() # Initialize the Model2 class as a subclass of nn.Module\n",
        "      self.fc1 = nn.Linear(28*28, 128)  # Input size: 28*28, Output size: 128\n",
        "      self.fc2 = nn.Linear(128, 10) # Input size: 128, Output size: 10\n",
        "\n",
        "  def forward(self, x): # Forward method defines the forward pass of the neural network\n",
        "      x = x.view(-1, 28*28) # Reshape the input tensor x to have dimensions (-1, 28*28)\n",
        "      x = self.fc1(x)  # Pass the input through fc1 without an activation function\n",
        "      x = torch.softmax(x, dim=1) # Apply the softmax activation function to the output of fc1 along dimension 1\n",
        "      x = self.fc2(x) # Pass the result through fc2 without an activation function\n",
        "      return x # Return the final output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGVkZD9SR4sV"
      },
      "source": [
        "In the code above, each model is configured with a different activation function: ReLU, Sigmoid, and Softmax. With this configuration, we can conduct experiments to determine which model is most suitable for the classification task at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k96nXSxUkMLD"
      },
      "source": [
        "### Loss Function menggunakan Cross Entropy loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIG-dWrEdcfn"
      },
      "source": [
        "With the configuration below, the model can be trained using a training approach suitable for the Cross Entropy Loss Function, with a specific learning rate, momentum, and number of epochs. This configuration is typically used in training approaches employing algorithms such as Stochastic Gradient Descent (SGD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVe19uRg4cf_"
      },
      "outputs": [],
      "source": [
        "cross_el = nn.CrossEntropyLoss() # Using Cross Entropy loss function\n",
        "learning_rate = 0.01 # Configuring the learning rate\n",
        "momentum = 0.9 # Configuring momentum\n",
        "num_epochs = 5 # Number of epochs or training iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLLJu9kfdhkK"
      },
      "source": [
        "The provided code below is for training and evaluating multiple neural network models using PyTorch. The training is done using Stochastic Gradient Descent (SGD) as the optimizer, and the models are trained for a specified number of epochs on a training dataset (train_loader). After training, the code evaluates the accuracy of each trained model on a separate test dataset (test_loader).  Here's a breakdown of the code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqSfckclFBei",
        "outputId": "bcc24422-e086-47ff-cc65-038c2161722a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1\n",
            "[1, 100] loss: 1.310\n",
            "[1, 200] loss: 0.687\n",
            "[1, 300] loss: 0.738\n",
            "[1, 400] loss: 0.621\n",
            "[1, 500] loss: 0.589\n",
            "[1, 600] loss: 0.550\n",
            "[1, 700] loss: 0.502\n",
            "[1, 800] loss: 0.538\n",
            "[1, 900] loss: 0.568\n",
            "[1, 1000] loss: 0.437\n",
            "[1, 1100] loss: 0.524\n",
            "[1, 1200] loss: 0.458\n",
            "[1, 1300] loss: 0.425\n",
            "[1, 1400] loss: 0.460\n",
            "[1, 1500] loss: 0.453\n",
            "[1, 1600] loss: 0.481\n",
            "[1, 1700] loss: 0.362\n",
            "[1, 1800] loss: 0.454\n",
            "[1, 1900] loss: 0.520\n",
            "[1, 2000] loss: 0.394\n",
            "[1, 2100] loss: 0.489\n",
            "[1, 2200] loss: 0.433\n",
            "[1, 2300] loss: 0.383\n",
            "[1, 2400] loss: 0.410\n",
            "[1, 2500] loss: 0.337\n",
            "[1, 2600] loss: 0.354\n",
            "[1, 2700] loss: 0.388\n",
            "[1, 2800] loss: 0.363\n",
            "[1, 2900] loss: 0.254\n",
            "[1, 3000] loss: 0.368\n",
            "[1, 3100] loss: 0.387\n",
            "[1, 3200] loss: 0.383\n",
            "[1, 3300] loss: 0.260\n",
            "[1, 3400] loss: 0.322\n",
            "[1, 3500] loss: 0.424\n",
            "[1, 3600] loss: 0.415\n",
            "[1, 3700] loss: 0.318\n",
            "[1, 3800] loss: 0.375\n",
            "[1, 3900] loss: 0.255\n",
            "[1, 4000] loss: 0.371\n",
            "[1, 4100] loss: 0.430\n",
            "[1, 4200] loss: 0.340\n",
            "[1, 4300] loss: 0.274\n",
            "[1, 4400] loss: 0.327\n",
            "[1, 4500] loss: 0.323\n",
            "[1, 4600] loss: 0.273\n",
            "[1, 4700] loss: 0.318\n",
            "[1, 4800] loss: 0.359\n",
            "[1, 4900] loss: 0.336\n",
            "[1, 5000] loss: 0.349\n",
            "[1, 5100] loss: 0.240\n",
            "[1, 5200] loss: 0.366\n",
            "[1, 5300] loss: 0.300\n",
            "[1, 5400] loss: 0.341\n",
            "[1, 5500] loss: 0.329\n",
            "[1, 5600] loss: 0.291\n",
            "[1, 5700] loss: 0.281\n",
            "[1, 5800] loss: 0.304\n",
            "[1, 5900] loss: 0.250\n",
            "[1, 6000] loss: 0.301\n",
            "[2, 100] loss: 0.282\n",
            "[2, 200] loss: 0.253\n",
            "[2, 300] loss: 0.254\n",
            "[2, 400] loss: 0.262\n",
            "[2, 500] loss: 0.258\n",
            "[2, 600] loss: 0.255\n",
            "[2, 700] loss: 0.207\n",
            "[2, 800] loss: 0.294\n",
            "[2, 900] loss: 0.265\n",
            "[2, 1000] loss: 0.226\n",
            "[2, 1100] loss: 0.254\n",
            "[2, 1200] loss: 0.285\n",
            "[2, 1300] loss: 0.269\n",
            "[2, 1400] loss: 0.289\n",
            "[2, 1500] loss: 0.236\n",
            "[2, 1600] loss: 0.311\n",
            "[2, 1700] loss: 0.254\n",
            "[2, 1800] loss: 0.256\n",
            "[2, 1900] loss: 0.259\n",
            "[2, 2000] loss: 0.173\n",
            "[2, 2100] loss: 0.244\n",
            "[2, 2200] loss: 0.239\n",
            "[2, 2300] loss: 0.336\n",
            "[2, 2400] loss: 0.296\n",
            "[2, 2500] loss: 0.264\n",
            "[2, 2600] loss: 0.282\n",
            "[2, 2700] loss: 0.248\n",
            "[2, 2800] loss: 0.248\n",
            "[2, 2900] loss: 0.214\n",
            "[2, 3000] loss: 0.286\n",
            "[2, 3100] loss: 0.253\n",
            "[2, 3200] loss: 0.222\n",
            "[2, 3300] loss: 0.251\n",
            "[2, 3400] loss: 0.259\n",
            "[2, 3500] loss: 0.223\n",
            "[2, 3600] loss: 0.240\n",
            "[2, 3700] loss: 0.291\n",
            "[2, 3800] loss: 0.276\n",
            "[2, 3900] loss: 0.276\n",
            "[2, 4000] loss: 0.202\n",
            "[2, 4100] loss: 0.305\n",
            "[2, 4200] loss: 0.307\n",
            "[2, 4300] loss: 0.265\n",
            "[2, 4400] loss: 0.301\n",
            "[2, 4500] loss: 0.331\n",
            "[2, 4600] loss: 0.294\n",
            "[2, 4700] loss: 0.292\n",
            "[2, 4800] loss: 0.196\n",
            "[2, 4900] loss: 0.270\n",
            "[2, 5000] loss: 0.209\n",
            "[2, 5100] loss: 0.184\n",
            "[2, 5200] loss: 0.256\n",
            "[2, 5300] loss: 0.284\n",
            "[2, 5400] loss: 0.223\n",
            "[2, 5500] loss: 0.277\n",
            "[2, 5600] loss: 0.234\n",
            "[2, 5700] loss: 0.261\n",
            "[2, 5800] loss: 0.205\n",
            "[2, 5900] loss: 0.213\n",
            "[2, 6000] loss: 0.264\n",
            "[3, 100] loss: 0.205\n",
            "[3, 200] loss: 0.234\n",
            "[3, 300] loss: 0.229\n",
            "[3, 400] loss: 0.205\n",
            "[3, 500] loss: 0.200\n",
            "[3, 600] loss: 0.208\n",
            "[3, 700] loss: 0.163\n",
            "[3, 800] loss: 0.197\n",
            "[3, 900] loss: 0.239\n",
            "[3, 1000] loss: 0.189\n",
            "[3, 1100] loss: 0.200\n",
            "[3, 1200] loss: 0.213\n",
            "[3, 1300] loss: 0.249\n",
            "[3, 1400] loss: 0.227\n",
            "[3, 1500] loss: 0.251\n",
            "[3, 1600] loss: 0.251\n",
            "[3, 1700] loss: 0.232\n",
            "[3, 1800] loss: 0.179\n",
            "[3, 1900] loss: 0.236\n",
            "[3, 2000] loss: 0.205\n",
            "[3, 2100] loss: 0.167\n",
            "[3, 2200] loss: 0.227\n",
            "[3, 2300] loss: 0.188\n",
            "[3, 2400] loss: 0.269\n",
            "[3, 2500] loss: 0.221\n",
            "[3, 2600] loss: 0.250\n",
            "[3, 2700] loss: 0.261\n",
            "[3, 2800] loss: 0.172\n",
            "[3, 2900] loss: 0.157\n",
            "[3, 3000] loss: 0.220\n",
            "[3, 3100] loss: 0.164\n",
            "[3, 3200] loss: 0.210\n",
            "[3, 3300] loss: 0.228\n",
            "[3, 3400] loss: 0.220\n",
            "[3, 3500] loss: 0.157\n",
            "[3, 3600] loss: 0.198\n",
            "[3, 3700] loss: 0.296\n",
            "[3, 3800] loss: 0.268\n",
            "[3, 3900] loss: 0.256\n",
            "[3, 4000] loss: 0.226\n",
            "[3, 4100] loss: 0.209\n",
            "[3, 4200] loss: 0.262\n",
            "[3, 4300] loss: 0.229\n",
            "[3, 4400] loss: 0.178\n",
            "[3, 4500] loss: 0.195\n",
            "[3, 4600] loss: 0.191\n",
            "[3, 4700] loss: 0.207\n",
            "[3, 4800] loss: 0.215\n",
            "[3, 4900] loss: 0.187\n",
            "[3, 5000] loss: 0.221\n",
            "[3, 5100] loss: 0.193\n",
            "[3, 5200] loss: 0.190\n",
            "[3, 5300] loss: 0.187\n",
            "[3, 5400] loss: 0.201\n",
            "[3, 5500] loss: 0.232\n",
            "[3, 5600] loss: 0.178\n",
            "[3, 5700] loss: 0.242\n",
            "[3, 5800] loss: 0.169\n",
            "[3, 5900] loss: 0.214\n",
            "[3, 6000] loss: 0.177\n",
            "[4, 100] loss: 0.144\n",
            "[4, 200] loss: 0.186\n",
            "[4, 300] loss: 0.195\n",
            "[4, 400] loss: 0.160\n",
            "[4, 500] loss: 0.198\n",
            "[4, 600] loss: 0.178\n",
            "[4, 700] loss: 0.168\n",
            "[4, 800] loss: 0.206\n",
            "[4, 900] loss: 0.132\n",
            "[4, 1000] loss: 0.207\n",
            "[4, 1100] loss: 0.220\n",
            "[4, 1200] loss: 0.157\n",
            "[4, 1300] loss: 0.166\n",
            "[4, 1400] loss: 0.148\n",
            "[4, 1500] loss: 0.158\n",
            "[4, 1600] loss: 0.227\n",
            "[4, 1700] loss: 0.188\n",
            "[4, 1800] loss: 0.163\n",
            "[4, 1900] loss: 0.177\n",
            "[4, 2000] loss: 0.205\n",
            "[4, 2100] loss: 0.124\n",
            "[4, 2200] loss: 0.285\n",
            "[4, 2300] loss: 0.203\n",
            "[4, 2400] loss: 0.192\n",
            "[4, 2500] loss: 0.169\n",
            "[4, 2600] loss: 0.262\n",
            "[4, 2700] loss: 0.209\n",
            "[4, 2800] loss: 0.147\n",
            "[4, 2900] loss: 0.194\n",
            "[4, 3000] loss: 0.200\n",
            "[4, 3100] loss: 0.192\n",
            "[4, 3200] loss: 0.220\n",
            "[4, 3300] loss: 0.201\n",
            "[4, 3400] loss: 0.219\n",
            "[4, 3500] loss: 0.185\n",
            "[4, 3600] loss: 0.179\n",
            "[4, 3700] loss: 0.158\n",
            "[4, 3800] loss: 0.182\n",
            "[4, 3900] loss: 0.165\n",
            "[4, 4000] loss: 0.158\n",
            "[4, 4100] loss: 0.183\n",
            "[4, 4200] loss: 0.225\n",
            "[4, 4300] loss: 0.219\n",
            "[4, 4400] loss: 0.207\n",
            "[4, 4500] loss: 0.152\n",
            "[4, 4600] loss: 0.231\n",
            "[4, 4700] loss: 0.151\n",
            "[4, 4800] loss: 0.198\n",
            "[4, 4900] loss: 0.217\n",
            "[4, 5000] loss: 0.250\n",
            "[4, 5100] loss: 0.140\n",
            "[4, 5200] loss: 0.131\n",
            "[4, 5300] loss: 0.208\n",
            "[4, 5400] loss: 0.185\n",
            "[4, 5500] loss: 0.148\n",
            "[4, 5600] loss: 0.207\n",
            "[4, 5700] loss: 0.163\n",
            "[4, 5800] loss: 0.225\n",
            "[4, 5900] loss: 0.201\n",
            "[4, 6000] loss: 0.266\n",
            "[5, 100] loss: 0.120\n",
            "[5, 200] loss: 0.189\n",
            "[5, 300] loss: 0.186\n",
            "[5, 400] loss: 0.222\n",
            "[5, 500] loss: 0.164\n",
            "[5, 600] loss: 0.179\n",
            "[5, 700] loss: 0.127\n",
            "[5, 800] loss: 0.200\n",
            "[5, 900] loss: 0.153\n",
            "[5, 1000] loss: 0.123\n",
            "[5, 1100] loss: 0.175\n",
            "[5, 1200] loss: 0.147\n",
            "[5, 1300] loss: 0.175\n",
            "[5, 1400] loss: 0.153\n",
            "[5, 1500] loss: 0.189\n",
            "[5, 1600] loss: 0.160\n",
            "[5, 1700] loss: 0.130\n",
            "[5, 1800] loss: 0.148\n",
            "[5, 1900] loss: 0.105\n",
            "[5, 2000] loss: 0.174\n",
            "[5, 2100] loss: 0.207\n",
            "[5, 2200] loss: 0.189\n",
            "[5, 2300] loss: 0.170\n",
            "[5, 2400] loss: 0.228\n",
            "[5, 2500] loss: 0.163\n",
            "[5, 2600] loss: 0.176\n",
            "[5, 2700] loss: 0.135\n",
            "[5, 2800] loss: 0.146\n",
            "[5, 2900] loss: 0.151\n",
            "[5, 3000] loss: 0.159\n",
            "[5, 3100] loss: 0.159\n",
            "[5, 3200] loss: 0.128\n",
            "[5, 3300] loss: 0.202\n",
            "[5, 3400] loss: 0.150\n",
            "[5, 3500] loss: 0.220\n",
            "[5, 3600] loss: 0.183\n",
            "[5, 3700] loss: 0.190\n",
            "[5, 3800] loss: 0.206\n",
            "[5, 3900] loss: 0.130\n",
            "[5, 4000] loss: 0.201\n",
            "[5, 4100] loss: 0.204\n",
            "[5, 4200] loss: 0.135\n",
            "[5, 4300] loss: 0.091\n",
            "[5, 4400] loss: 0.186\n",
            "[5, 4500] loss: 0.203\n",
            "[5, 4600] loss: 0.155\n",
            "[5, 4700] loss: 0.194\n",
            "[5, 4800] loss: 0.256\n",
            "[5, 4900] loss: 0.126\n",
            "[5, 5000] loss: 0.263\n",
            "[5, 5100] loss: 0.235\n",
            "[5, 5200] loss: 0.151\n",
            "[5, 5300] loss: 0.149\n",
            "[5, 5400] loss: 0.186\n",
            "[5, 5500] loss: 0.131\n",
            "[5, 5600] loss: 0.146\n",
            "[5, 5700] loss: 0.200\n",
            "[5, 5800] loss: 0.190\n",
            "[5, 5900] loss: 0.192\n",
            "[5, 6000] loss: 0.156\n",
            "Accuracy: 95.24%\n",
            "Model 2\n",
            "[1, 100] loss: 1.983\n",
            "[1, 200] loss: 1.041\n",
            "[1, 300] loss: 0.724\n",
            "[1, 400] loss: 0.610\n",
            "[1, 500] loss: 0.499\n",
            "[1, 600] loss: 0.456\n",
            "[1, 700] loss: 0.478\n",
            "[1, 800] loss: 0.468\n",
            "[1, 900] loss: 0.372\n",
            "[1, 1000] loss: 0.344\n",
            "[1, 1100] loss: 0.375\n",
            "[1, 1200] loss: 0.408\n",
            "[1, 1300] loss: 0.364\n",
            "[1, 1400] loss: 0.398\n",
            "[1, 1500] loss: 0.356\n",
            "[1, 1600] loss: 0.392\n",
            "[1, 1700] loss: 0.372\n",
            "[1, 1800] loss: 0.313\n",
            "[1, 1900] loss: 0.325\n",
            "[1, 2000] loss: 0.362\n",
            "[1, 2100] loss: 0.319\n",
            "[1, 2200] loss: 0.253\n",
            "[1, 2300] loss: 0.303\n",
            "[1, 2400] loss: 0.363\n",
            "[1, 2500] loss: 0.308\n",
            "[1, 2600] loss: 0.301\n",
            "[1, 2700] loss: 0.295\n",
            "[1, 2800] loss: 0.305\n",
            "[1, 2900] loss: 0.301\n",
            "[1, 3000] loss: 0.326\n",
            "[1, 3100] loss: 0.310\n",
            "[1, 3200] loss: 0.296\n",
            "[1, 3300] loss: 0.243\n",
            "[1, 3400] loss: 0.249\n",
            "[1, 3500] loss: 0.272\n",
            "[1, 3600] loss: 0.262\n",
            "[1, 3700] loss: 0.312\n",
            "[1, 3800] loss: 0.227\n",
            "[1, 3900] loss: 0.247\n",
            "[1, 4000] loss: 0.252\n",
            "[1, 4100] loss: 0.248\n",
            "[1, 4200] loss: 0.235\n",
            "[1, 4300] loss: 0.217\n",
            "[1, 4400] loss: 0.233\n",
            "[1, 4500] loss: 0.206\n",
            "[1, 4600] loss: 0.237\n",
            "[1, 4700] loss: 0.221\n",
            "[1, 4800] loss: 0.223\n",
            "[1, 4900] loss: 0.231\n",
            "[1, 5000] loss: 0.223\n",
            "[1, 5100] loss: 0.198\n",
            "[1, 5200] loss: 0.230\n",
            "[1, 5300] loss: 0.228\n",
            "[1, 5400] loss: 0.192\n",
            "[1, 5500] loss: 0.250\n",
            "[1, 5600] loss: 0.193\n",
            "[1, 5700] loss: 0.155\n",
            "[1, 5800] loss: 0.209\n",
            "[1, 5900] loss: 0.216\n",
            "[1, 6000] loss: 0.210\n",
            "[2, 100] loss: 0.121\n",
            "[2, 200] loss: 0.211\n",
            "[2, 300] loss: 0.165\n",
            "[2, 400] loss: 0.162\n",
            "[2, 500] loss: 0.201\n",
            "[2, 600] loss: 0.164\n",
            "[2, 700] loss: 0.164\n",
            "[2, 800] loss: 0.203\n",
            "[2, 900] loss: 0.177\n",
            "[2, 1000] loss: 0.203\n",
            "[2, 1100] loss: 0.195\n",
            "[2, 1200] loss: 0.236\n",
            "[2, 1300] loss: 0.178\n",
            "[2, 1400] loss: 0.146\n",
            "[2, 1500] loss: 0.189\n",
            "[2, 1600] loss: 0.172\n",
            "[2, 1700] loss: 0.150\n",
            "[2, 1800] loss: 0.163\n",
            "[2, 1900] loss: 0.185\n",
            "[2, 2000] loss: 0.171\n",
            "[2, 2100] loss: 0.182\n",
            "[2, 2200] loss: 0.201\n",
            "[2, 2300] loss: 0.185\n",
            "[2, 2400] loss: 0.170\n",
            "[2, 2500] loss: 0.157\n",
            "[2, 2600] loss: 0.147\n",
            "[2, 2700] loss: 0.167\n",
            "[2, 2800] loss: 0.151\n",
            "[2, 2900] loss: 0.187\n",
            "[2, 3000] loss: 0.154\n",
            "[2, 3100] loss: 0.178\n",
            "[2, 3200] loss: 0.148\n",
            "[2, 3300] loss: 0.186\n",
            "[2, 3400] loss: 0.143\n",
            "[2, 3500] loss: 0.198\n",
            "[2, 3600] loss: 0.170\n",
            "[2, 3700] loss: 0.187\n",
            "[2, 3800] loss: 0.156\n",
            "[2, 3900] loss: 0.199\n",
            "[2, 4000] loss: 0.172\n",
            "[2, 4100] loss: 0.178\n",
            "[2, 4200] loss: 0.154\n",
            "[2, 4300] loss: 0.145\n",
            "[2, 4400] loss: 0.133\n",
            "[2, 4500] loss: 0.162\n",
            "[2, 4600] loss: 0.178\n",
            "[2, 4700] loss: 0.141\n",
            "[2, 4800] loss: 0.115\n",
            "[2, 4900] loss: 0.147\n",
            "[2, 5000] loss: 0.167\n",
            "[2, 5100] loss: 0.144\n",
            "[2, 5200] loss: 0.152\n",
            "[2, 5300] loss: 0.136\n",
            "[2, 5400] loss: 0.146\n",
            "[2, 5500] loss: 0.175\n",
            "[2, 5600] loss: 0.155\n",
            "[2, 5700] loss: 0.134\n",
            "[2, 5800] loss: 0.177\n",
            "[2, 5900] loss: 0.136\n",
            "[2, 6000] loss: 0.111\n",
            "[3, 100] loss: 0.122\n",
            "[3, 200] loss: 0.157\n",
            "[3, 300] loss: 0.115\n",
            "[3, 400] loss: 0.112\n",
            "[3, 500] loss: 0.118\n",
            "[3, 600] loss: 0.106\n",
            "[3, 700] loss: 0.125\n",
            "[3, 800] loss: 0.132\n",
            "[3, 900] loss: 0.139\n",
            "[3, 1000] loss: 0.123\n",
            "[3, 1100] loss: 0.138\n",
            "[3, 1200] loss: 0.122\n",
            "[3, 1300] loss: 0.134\n",
            "[3, 1400] loss: 0.120\n",
            "[3, 1500] loss: 0.099\n",
            "[3, 1600] loss: 0.126\n",
            "[3, 1700] loss: 0.118\n",
            "[3, 1800] loss: 0.108\n",
            "[3, 1900] loss: 0.143\n",
            "[3, 2000] loss: 0.109\n",
            "[3, 2100] loss: 0.098\n",
            "[3, 2200] loss: 0.117\n",
            "[3, 2300] loss: 0.130\n",
            "[3, 2400] loss: 0.146\n",
            "[3, 2500] loss: 0.139\n",
            "[3, 2600] loss: 0.157\n",
            "[3, 2700] loss: 0.096\n",
            "[3, 2800] loss: 0.098\n",
            "[3, 2900] loss: 0.144\n",
            "[3, 3000] loss: 0.123\n",
            "[3, 3100] loss: 0.099\n",
            "[3, 3200] loss: 0.130\n",
            "[3, 3300] loss: 0.116\n",
            "[3, 3400] loss: 0.151\n",
            "[3, 3500] loss: 0.126\n",
            "[3, 3600] loss: 0.130\n",
            "[3, 3700] loss: 0.158\n",
            "[3, 3800] loss: 0.131\n",
            "[3, 3900] loss: 0.116\n",
            "[3, 4000] loss: 0.076\n",
            "[3, 4100] loss: 0.100\n",
            "[3, 4200] loss: 0.095\n",
            "[3, 4300] loss: 0.108\n",
            "[3, 4400] loss: 0.125\n",
            "[3, 4500] loss: 0.133\n",
            "[3, 4600] loss: 0.139\n",
            "[3, 4700] loss: 0.117\n",
            "[3, 4800] loss: 0.132\n",
            "[3, 4900] loss: 0.120\n",
            "[3, 5000] loss: 0.121\n",
            "[3, 5100] loss: 0.131\n",
            "[3, 5200] loss: 0.118\n",
            "[3, 5300] loss: 0.120\n",
            "[3, 5400] loss: 0.106\n",
            "[3, 5500] loss: 0.106\n",
            "[3, 5600] loss: 0.127\n",
            "[3, 5700] loss: 0.100\n",
            "[3, 5800] loss: 0.102\n",
            "[3, 5900] loss: 0.088\n",
            "[3, 6000] loss: 0.166\n",
            "[4, 100] loss: 0.106\n",
            "[4, 200] loss: 0.118\n",
            "[4, 300] loss: 0.110\n",
            "[4, 400] loss: 0.095\n",
            "[4, 500] loss: 0.105\n",
            "[4, 600] loss: 0.127\n",
            "[4, 700] loss: 0.072\n",
            "[4, 800] loss: 0.071\n",
            "[4, 900] loss: 0.098\n",
            "[4, 1000] loss: 0.092\n",
            "[4, 1100] loss: 0.113\n",
            "[4, 1200] loss: 0.097\n",
            "[4, 1300] loss: 0.082\n",
            "[4, 1400] loss: 0.068\n",
            "[4, 1500] loss: 0.089\n",
            "[4, 1600] loss: 0.108\n",
            "[4, 1700] loss: 0.109\n",
            "[4, 1800] loss: 0.098\n",
            "[4, 1900] loss: 0.098\n",
            "[4, 2000] loss: 0.110\n",
            "[4, 2100] loss: 0.120\n",
            "[4, 2200] loss: 0.097\n",
            "[4, 2300] loss: 0.108\n",
            "[4, 2400] loss: 0.148\n",
            "[4, 2500] loss: 0.116\n",
            "[4, 2600] loss: 0.107\n",
            "[4, 2700] loss: 0.067\n",
            "[4, 2800] loss: 0.122\n",
            "[4, 2900] loss: 0.095\n",
            "[4, 3000] loss: 0.099\n",
            "[4, 3100] loss: 0.094\n",
            "[4, 3200] loss: 0.087\n",
            "[4, 3300] loss: 0.094\n",
            "[4, 3400] loss: 0.092\n",
            "[4, 3500] loss: 0.099\n",
            "[4, 3600] loss: 0.085\n",
            "[4, 3700] loss: 0.138\n",
            "[4, 3800] loss: 0.102\n",
            "[4, 3900] loss: 0.082\n",
            "[4, 4000] loss: 0.081\n",
            "[4, 4100] loss: 0.078\n",
            "[4, 4200] loss: 0.155\n",
            "[4, 4300] loss: 0.086\n",
            "[4, 4400] loss: 0.087\n",
            "[4, 4500] loss: 0.106\n",
            "[4, 4600] loss: 0.097\n",
            "[4, 4700] loss: 0.116\n",
            "[4, 4800] loss: 0.119\n",
            "[4, 4900] loss: 0.101\n",
            "[4, 5000] loss: 0.114\n",
            "[4, 5100] loss: 0.095\n",
            "[4, 5200] loss: 0.093\n",
            "[4, 5300] loss: 0.096\n",
            "[4, 5400] loss: 0.076\n",
            "[4, 5500] loss: 0.102\n",
            "[4, 5600] loss: 0.075\n",
            "[4, 5700] loss: 0.099\n",
            "[4, 5800] loss: 0.097\n",
            "[4, 5900] loss: 0.094\n",
            "[4, 6000] loss: 0.095\n",
            "[5, 100] loss: 0.056\n",
            "[5, 200] loss: 0.062\n",
            "[5, 300] loss: 0.111\n",
            "[5, 400] loss: 0.084\n",
            "[5, 500] loss: 0.080\n",
            "[5, 600] loss: 0.063\n",
            "[5, 700] loss: 0.069\n",
            "[5, 800] loss: 0.093\n",
            "[5, 900] loss: 0.071\n",
            "[5, 1000] loss: 0.079\n",
            "[5, 1100] loss: 0.106\n",
            "[5, 1200] loss: 0.081\n",
            "[5, 1300] loss: 0.097\n",
            "[5, 1400] loss: 0.088\n",
            "[5, 1500] loss: 0.072\n",
            "[5, 1600] loss: 0.085\n",
            "[5, 1700] loss: 0.096\n",
            "[5, 1800] loss: 0.089\n",
            "[5, 1900] loss: 0.081\n",
            "[5, 2000] loss: 0.096\n",
            "[5, 2100] loss: 0.098\n",
            "[5, 2200] loss: 0.071\n",
            "[5, 2300] loss: 0.081\n",
            "[5, 2400] loss: 0.074\n",
            "[5, 2500] loss: 0.077\n",
            "[5, 2600] loss: 0.075\n",
            "[5, 2700] loss: 0.080\n",
            "[5, 2800] loss: 0.079\n",
            "[5, 2900] loss: 0.076\n",
            "[5, 3000] loss: 0.070\n",
            "[5, 3100] loss: 0.081\n",
            "[5, 3200] loss: 0.086\n",
            "[5, 3300] loss: 0.091\n",
            "[5, 3400] loss: 0.097\n",
            "[5, 3500] loss: 0.104\n",
            "[5, 3600] loss: 0.097\n",
            "[5, 3700] loss: 0.075\n",
            "[5, 3800] loss: 0.141\n",
            "[5, 3900] loss: 0.105\n",
            "[5, 4000] loss: 0.078\n",
            "[5, 4100] loss: 0.105\n",
            "[5, 4200] loss: 0.073\n",
            "[5, 4300] loss: 0.083\n",
            "[5, 4400] loss: 0.056\n",
            "[5, 4500] loss: 0.062\n",
            "[5, 4600] loss: 0.079\n",
            "[5, 4700] loss: 0.062\n",
            "[5, 4800] loss: 0.086\n",
            "[5, 4900] loss: 0.071\n",
            "[5, 5000] loss: 0.067\n",
            "[5, 5100] loss: 0.079\n",
            "[5, 5200] loss: 0.091\n",
            "[5, 5300] loss: 0.077\n",
            "[5, 5400] loss: 0.095\n",
            "[5, 5500] loss: 0.082\n",
            "[5, 5600] loss: 0.090\n",
            "[5, 5700] loss: 0.061\n",
            "[5, 5800] loss: 0.071\n",
            "[5, 5900] loss: 0.079\n",
            "[5, 6000] loss: 0.095\n",
            "Accuracy: 97.0%\n",
            "Model 3\n",
            "[1, 100] loss: 2.304\n",
            "[1, 200] loss: 2.302\n",
            "[1, 300] loss: 2.292\n",
            "[1, 400] loss: 2.240\n",
            "[1, 500] loss: 2.143\n",
            "[1, 600] loss: 2.056\n",
            "[1, 700] loss: 1.979\n",
            "[1, 800] loss: 1.893\n",
            "[1, 900] loss: 1.801\n",
            "[1, 1000] loss: 1.715\n",
            "[1, 1100] loss: 1.706\n",
            "[1, 1200] loss: 1.675\n",
            "[1, 1300] loss: 1.631\n",
            "[1, 1400] loss: 1.688\n",
            "[1, 1500] loss: 1.644\n",
            "[1, 1600] loss: 1.587\n",
            "[1, 1700] loss: 1.667\n",
            "[1, 1800] loss: 1.641\n",
            "[1, 1900] loss: 1.606\n",
            "[1, 2000] loss: 1.655\n",
            "[1, 2100] loss: 1.594\n",
            "[1, 2200] loss: 1.580\n",
            "[1, 2300] loss: 1.558\n",
            "[1, 2400] loss: 1.528\n",
            "[1, 2500] loss: 1.534\n",
            "[1, 2600] loss: 1.517\n",
            "[1, 2700] loss: 1.555\n",
            "[1, 2800] loss: 1.584\n",
            "[1, 2900] loss: 1.571\n",
            "[1, 3000] loss: 1.488\n",
            "[1, 3100] loss: 1.542\n",
            "[1, 3200] loss: 1.534\n",
            "[1, 3300] loss: 1.534\n",
            "[1, 3400] loss: 1.519\n",
            "[1, 3500] loss: 1.456\n",
            "[1, 3600] loss: 1.502\n",
            "[1, 3700] loss: 1.470\n",
            "[1, 3800] loss: 1.448\n",
            "[1, 3900] loss: 1.453\n",
            "[1, 4000] loss: 1.465\n",
            "[1, 4100] loss: 1.449\n",
            "[1, 4200] loss: 1.410\n",
            "[1, 4300] loss: 1.481\n",
            "[1, 4400] loss: 1.401\n",
            "[1, 4500] loss: 1.369\n",
            "[1, 4600] loss: 1.494\n",
            "[1, 4700] loss: 1.418\n",
            "[1, 4800] loss: 1.426\n",
            "[1, 4900] loss: 1.390\n",
            "[1, 5000] loss: 1.436\n",
            "[1, 5100] loss: 1.416\n",
            "[1, 5200] loss: 1.389\n",
            "[1, 5300] loss: 1.384\n",
            "[1, 5400] loss: 1.384\n",
            "[1, 5500] loss: 1.352\n",
            "[1, 5600] loss: 1.354\n",
            "[1, 5700] loss: 1.378\n",
            "[1, 5800] loss: 1.338\n",
            "[1, 5900] loss: 1.400\n",
            "[1, 6000] loss: 1.389\n",
            "[2, 100] loss: 1.310\n",
            "[2, 200] loss: 1.377\n",
            "[2, 300] loss: 1.377\n",
            "[2, 400] loss: 1.321\n",
            "[2, 500] loss: 1.388\n",
            "[2, 600] loss: 1.325\n",
            "[2, 700] loss: 1.306\n",
            "[2, 800] loss: 1.359\n",
            "[2, 900] loss: 1.247\n",
            "[2, 1000] loss: 1.310\n",
            "[2, 1100] loss: 1.262\n",
            "[2, 1200] loss: 1.236\n",
            "[2, 1300] loss: 1.163\n",
            "[2, 1400] loss: 1.166\n",
            "[2, 1500] loss: 1.154\n",
            "[2, 1600] loss: 1.186\n",
            "[2, 1700] loss: 1.187\n",
            "[2, 1800] loss: 1.189\n",
            "[2, 1900] loss: 1.140\n",
            "[2, 2000] loss: 1.178\n",
            "[2, 2100] loss: 1.184\n",
            "[2, 2200] loss: 1.094\n",
            "[2, 2300] loss: 1.135\n",
            "[2, 2400] loss: 1.078\n",
            "[2, 2500] loss: 1.176\n",
            "[2, 2600] loss: 1.140\n",
            "[2, 2700] loss: 1.138\n",
            "[2, 2800] loss: 1.077\n",
            "[2, 2900] loss: 1.135\n",
            "[2, 3000] loss: 1.070\n",
            "[2, 3100] loss: 1.113\n",
            "[2, 3200] loss: 1.100\n",
            "[2, 3300] loss: 1.096\n",
            "[2, 3400] loss: 1.107\n",
            "[2, 3500] loss: 1.157\n",
            "[2, 3600] loss: 1.062\n",
            "[2, 3700] loss: 1.082\n",
            "[2, 3800] loss: 1.148\n",
            "[2, 3900] loss: 1.030\n",
            "[2, 4000] loss: 1.059\n",
            "[2, 4100] loss: 1.098\n",
            "[2, 4200] loss: 1.103\n",
            "[2, 4300] loss: 1.110\n",
            "[2, 4400] loss: 1.136\n",
            "[2, 4500] loss: 1.108\n",
            "[2, 4600] loss: 1.132\n",
            "[2, 4700] loss: 1.066\n",
            "[2, 4800] loss: 1.061\n",
            "[2, 4900] loss: 1.070\n",
            "[2, 5000] loss: 1.091\n",
            "[2, 5100] loss: 1.079\n",
            "[2, 5200] loss: 1.153\n",
            "[2, 5300] loss: 1.107\n",
            "[2, 5400] loss: 1.091\n",
            "[2, 5500] loss: 1.066\n",
            "[2, 5600] loss: 1.035\n",
            "[2, 5700] loss: 1.137\n",
            "[2, 5800] loss: 1.083\n",
            "[2, 5900] loss: 1.066\n",
            "[2, 6000] loss: 1.089\n",
            "[3, 100] loss: 1.191\n",
            "[3, 200] loss: 1.144\n",
            "[3, 300] loss: 1.047\n",
            "[3, 400] loss: 1.074\n",
            "[3, 500] loss: 1.031\n",
            "[3, 600] loss: 1.145\n",
            "[3, 700] loss: 1.047\n",
            "[3, 800] loss: 1.032\n",
            "[3, 900] loss: 1.020\n",
            "[3, 1000] loss: 1.028\n",
            "[3, 1100] loss: 1.031\n",
            "[3, 1200] loss: 1.052\n",
            "[3, 1300] loss: 1.027\n",
            "[3, 1400] loss: 0.996\n",
            "[3, 1500] loss: 1.042\n",
            "[3, 1600] loss: 1.081\n",
            "[3, 1700] loss: 1.067\n",
            "[3, 1800] loss: 1.010\n",
            "[3, 1900] loss: 1.009\n",
            "[3, 2000] loss: 1.013\n",
            "[3, 2100] loss: 0.949\n",
            "[3, 2200] loss: 1.031\n",
            "[3, 2300] loss: 1.046\n",
            "[3, 2400] loss: 0.997\n",
            "[3, 2500] loss: 0.984\n",
            "[3, 2600] loss: 0.959\n",
            "[3, 2700] loss: 1.007\n",
            "[3, 2800] loss: 0.988\n",
            "[3, 2900] loss: 0.935\n",
            "[3, 3000] loss: 1.010\n",
            "[3, 3100] loss: 0.936\n",
            "[3, 3200] loss: 0.965\n",
            "[3, 3300] loss: 0.909\n",
            "[3, 3400] loss: 0.929\n",
            "[3, 3500] loss: 0.947\n",
            "[3, 3600] loss: 0.988\n",
            "[3, 3700] loss: 0.937\n",
            "[3, 3800] loss: 0.894\n",
            "[3, 3900] loss: 0.886\n",
            "[3, 4000] loss: 0.930\n",
            "[3, 4100] loss: 0.887\n",
            "[3, 4200] loss: 0.901\n",
            "[3, 4300] loss: 0.914\n",
            "[3, 4400] loss: 0.909\n",
            "[3, 4500] loss: 0.920\n",
            "[3, 4600] loss: 0.910\n",
            "[3, 4700] loss: 0.842\n",
            "[3, 4800] loss: 0.956\n",
            "[3, 4900] loss: 0.851\n",
            "[3, 5000] loss: 0.951\n",
            "[3, 5100] loss: 0.957\n",
            "[3, 5200] loss: 0.923\n",
            "[3, 5300] loss: 0.863\n",
            "[3, 5400] loss: 0.865\n",
            "[3, 5500] loss: 0.888\n",
            "[3, 5600] loss: 0.880\n",
            "[3, 5700] loss: 0.898\n",
            "[3, 5800] loss: 0.859\n",
            "[3, 5900] loss: 0.878\n",
            "[3, 6000] loss: 0.836\n",
            "[4, 100] loss: 0.761\n",
            "[4, 200] loss: 0.889\n",
            "[4, 300] loss: 0.880\n",
            "[4, 400] loss: 0.902\n",
            "[4, 500] loss: 0.897\n",
            "[4, 600] loss: 0.910\n",
            "[4, 700] loss: 0.845\n",
            "[4, 800] loss: 0.881\n",
            "[4, 900] loss: 0.866\n",
            "[4, 1000] loss: 0.884\n",
            "[4, 1100] loss: 0.821\n",
            "[4, 1200] loss: 0.866\n",
            "[4, 1300] loss: 0.845\n",
            "[4, 1400] loss: 0.857\n",
            "[4, 1500] loss: 0.965\n",
            "[4, 1600] loss: 0.918\n",
            "[4, 1700] loss: 0.832\n",
            "[4, 1800] loss: 0.892\n",
            "[4, 1900] loss: 0.853\n",
            "[4, 2000] loss: 0.900\n",
            "[4, 2100] loss: 0.897\n",
            "[4, 2200] loss: 0.869\n",
            "[4, 2300] loss: 0.816\n",
            "[4, 2400] loss: 0.863\n",
            "[4, 2500] loss: 0.808\n",
            "[4, 2600] loss: 0.795\n",
            "[4, 2700] loss: 0.819\n",
            "[4, 2800] loss: 0.940\n",
            "[4, 2900] loss: 0.900\n",
            "[4, 3000] loss: 0.849\n",
            "[4, 3100] loss: 0.877\n",
            "[4, 3200] loss: 0.909\n",
            "[4, 3300] loss: 0.888\n",
            "[4, 3400] loss: 0.862\n",
            "[4, 3500] loss: 0.838\n",
            "[4, 3600] loss: 0.857\n",
            "[4, 3700] loss: 0.850\n",
            "[4, 3800] loss: 0.832\n",
            "[4, 3900] loss: 0.899\n",
            "[4, 4000] loss: 0.880\n",
            "[4, 4100] loss: 0.847\n",
            "[4, 4200] loss: 0.878\n",
            "[4, 4300] loss: 0.812\n",
            "[4, 4400] loss: 0.891\n",
            "[4, 4500] loss: 0.885\n",
            "[4, 4600] loss: 0.777\n",
            "[4, 4700] loss: 0.862\n",
            "[4, 4800] loss: 0.892\n",
            "[4, 4900] loss: 0.919\n",
            "[4, 5000] loss: 0.868\n",
            "[4, 5100] loss: 0.854\n",
            "[4, 5200] loss: 0.819\n",
            "[4, 5300] loss: 0.787\n",
            "[4, 5400] loss: 0.826\n",
            "[4, 5500] loss: 0.856\n",
            "[4, 5600] loss: 0.874\n",
            "[4, 5700] loss: 0.869\n",
            "[4, 5800] loss: 0.876\n",
            "[4, 5900] loss: 0.884\n",
            "[4, 6000] loss: 0.784\n",
            "[5, 100] loss: 0.814\n",
            "[5, 200] loss: 0.778\n",
            "[5, 300] loss: 0.818\n",
            "[5, 400] loss: 0.828\n",
            "[5, 500] loss: 0.910\n",
            "[5, 600] loss: 0.869\n",
            "[5, 700] loss: 0.882\n",
            "[5, 800] loss: 0.832\n",
            "[5, 900] loss: 0.811\n",
            "[5, 1000] loss: 0.883\n",
            "[5, 1100] loss: 0.828\n",
            "[5, 1200] loss: 0.902\n",
            "[5, 1300] loss: 0.871\n",
            "[5, 1400] loss: 0.862\n",
            "[5, 1500] loss: 0.851\n",
            "[5, 1600] loss: 0.849\n",
            "[5, 1700] loss: 0.883\n",
            "[5, 1800] loss: 0.739\n",
            "[5, 1900] loss: 0.895\n",
            "[5, 2000] loss: 0.892\n",
            "[5, 2100] loss: 0.821\n",
            "[5, 2200] loss: 0.776\n",
            "[5, 2300] loss: 0.814\n",
            "[5, 2400] loss: 0.797\n",
            "[5, 2500] loss: 0.728\n",
            "[5, 2600] loss: 0.800\n",
            "[5, 2700] loss: 0.829\n",
            "[5, 2800] loss: 0.852\n",
            "[5, 2900] loss: 0.780\n",
            "[5, 3000] loss: 0.708\n",
            "[5, 3100] loss: 0.736\n",
            "[5, 3200] loss: 0.702\n",
            "[5, 3300] loss: 0.770\n",
            "[5, 3400] loss: 0.785\n",
            "[5, 3500] loss: 0.784\n",
            "[5, 3600] loss: 0.703\n",
            "[5, 3700] loss: 0.770\n",
            "[5, 3800] loss: 0.730\n",
            "[5, 3900] loss: 0.697\n",
            "[5, 4000] loss: 0.689\n",
            "[5, 4100] loss: 0.752\n",
            "[5, 4200] loss: 0.720\n",
            "[5, 4300] loss: 0.725\n",
            "[5, 4400] loss: 0.737\n",
            "[5, 4500] loss: 0.692\n",
            "[5, 4600] loss: 0.618\n",
            "[5, 4700] loss: 0.697\n",
            "[5, 4800] loss: 0.687\n",
            "[5, 4900] loss: 0.679\n",
            "[5, 5000] loss: 0.641\n",
            "[5, 5100] loss: 0.683\n",
            "[5, 5200] loss: 0.658\n",
            "[5, 5300] loss: 0.745\n",
            "[5, 5400] loss: 0.647\n",
            "[5, 5500] loss: 0.724\n",
            "[5, 5600] loss: 0.703\n",
            "[5, 5700] loss: 0.738\n",
            "[5, 5800] loss: 0.648\n",
            "[5, 5900] loss: 0.776\n",
            "[5, 6000] loss: 0.728\n",
            "Accuracy: 73.57%\n",
            "Accuracies: [95.24, 97.0, 73.57]\n"
          ]
        }
      ],
      "source": [
        "models = [Model1(), Model2(), Model3()] # Create instances of three different neural network models\n",
        "accuracies = [] # Create an empty list to store accuracy values for each model\n",
        "\n",
        "# Loop through each model\n",
        "for i, model in enumerate(models):\n",
        "  print(f\"Model {i+1}\")\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum) # Define the optimizer for Stochastic Gradient Descent (SGD)\n",
        "\n",
        "  # Loop through the specified number of epochs for training\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    # Loop through batches of data in the training loader\n",
        "    for j, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad() # Zero the gradients to prevent accumulation\n",
        "\n",
        "        outputs = model(inputs) # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        loss = cross_el(outputs, labels) # Compute the Cross Entropy loss\n",
        "        loss.backward() # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "        optimizer.step() # Perform a single optimization step (parameter update)\n",
        "\n",
        "        running_loss += loss.item()# Accumulate running loss\n",
        "        # Print running loss every 100 batches\n",
        "        if j % 100 == 99:\n",
        "            print(f\"[{epoch+1}, {j+1}] loss: {running_loss/100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "  # Evaluate the model on the test set\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "          images, labels = data\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1) # Get predicted labels\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item() # Count correct predictions\n",
        "\n",
        "  accuracy = 100 * correct / total # Calculate and print the accuracy of the model on the test set\n",
        "  print(f\"Accuracy: {accuracy}%\") # Print the accuracy of each models\n",
        "  accuracies.append(accuracy) # Append accuracy to the list\n",
        "\n",
        "print(\"Accuracies:\", accuracies) # Print the accuracies of all models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1P15yeGgfvM"
      },
      "source": [
        "In summary, this code trains and evaluates three different neural network models (Model1, Model2, Model3) using Stochastic Gradient Descent (SGD) as the optimizer and Cross Entropy loss as the loss function. It prints the loss during training and the accuracy on the test data for each model, and finally, it prints the accuracies of all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJN_oK30kqKW",
        "outputId": "8945954c-fab1-4724-e064-fe9197275a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracies: [95.24, 97.0, 73.57]\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracies:\", accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mqh5AZOkOAO"
      },
      "source": [
        "###Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1** <br></br>\n",
        "![Screenshot 2023-10-01 214610.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "S2ZwDZ5etHca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2** <br></br>\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "fCZtkEt6tmyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 3**\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "zajx14l-tp3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy results reveal compelling insights into the performance of each model with distinct activation functions:\n",
        "\n",
        "**Model 1 (ReLU Activation): 95.24%**\n",
        "The ReLU activation function demonstrates robust performance, achieving an accuracy of 95.24%. This is indicative of its ability to facilitate convergence during training, contributing to the model's overall efficacy in capturing intricate patterns within the data.\n",
        "\n",
        "**Model 2 (Sigmoid Activation): 97.0%**\n",
        "The superior performance of the model utilizing the sigmoid activation function, with an accuracy of 97.0%, suggests that sigmoid is particularly effective in binary classification scenarios. Its capacity to squash outputs into a range between 0 and 1 is advantageous for tasks where a probability-like interpretation is beneficial.\n",
        "\n",
        "**Model 3 (Softmax Activation): 73.57%**\n",
        "The Softmax activation function yields the least favorable performance, with an accuracy of 73.57%. Softmax is commonly employed in multi-class classification, but its tendency to accentuate differences between classes might not align optimally with the specific characteristics of the dataset, leading to suboptimal performance.\n",
        "\n",
        "From this evaluation, it is evident that the model using the **sigmoid activation function performs the best with an accuracy of 97.0%.** The model with ReLU activation function also performs well with an accuracy of 95.24%, while the model with Softmax activation function shows the lowest performance with an accuracy of 73.57%. This indicates that the choice of activation function can significantly influence model performance, underscoring the importance of selecting the appropriate activation function for achieving good results in classification tasks."
      ],
      "metadata": {
        "id": "h8GbwtqYt8qQ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}